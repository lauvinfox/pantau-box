{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "848bccc7-2d20-40af-8cce-d0fa2df4e987",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc39eff-e9fc-4e8e-b28e-f01a46c11ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef8da7e-95d7-47e9-a636-3cfcbf79381b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_vggface\n",
    "import keras_vggface.utils\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras import Model\n",
    "from keras import layers\n",
    "from keras.layers import Flatten, Input, Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f0efa7-4f23-4146-8bfb-4a979e4f946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mtcnn\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from keras_vggface.utils import preprocess_input\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30e31db-b7a7-469b-9f3b-972e1ba7b8a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261911ce-391e-4f78-8d7c-069e2e2fb9f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a037c1b-7988-4b2a-ade8-55ead5221cce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187f4140-ea77-43b4-a01c-85eceff7d7d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    keras.layers.RandomFlip('horizontal'),\n",
    "    keras.layers.RandomRotation(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44203cfa-f345-4f65-9bb7-d6a30dd518c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose `avg_pool` as the last layer of your base model\n",
    "last_layer = model.get_layer('avg_pool')\n",
    "print('last layer output shape: ', last_layer.output_shape)\n",
    "last_output = last_layer.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04b3be9-4821-4945-ae24-09a1996e4a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(224, 224, 3))\n",
    "\n",
    "x = data_augmentation(inputs)\n",
    "x = model(x)\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(1024, activation='relu', name='fc1')(x)\n",
    "x = Dense(1024, activation='relu', name='fc2')(x)\n",
    "x = Dense(1024, activation='relu', name='fc3')(x) \n",
    "x = Dense(1024, activation='relu', name='fc4')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(512, activation='relu', name='fc5')(x)\n",
    "x = Dense(512, activation='relu', name='fc6')(x) \n",
    "x = Dense(512, activation='relu', name='fc7')(x) \n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(256, activation='relu', name='fc8')(x)\n",
    "x = Dense(256, activation='relu', name='fc9')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(128, activation='relu', name='fc10')(x)\n",
    "out = Dense(105, name='classifier')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0938b082-a91d-4d3e-af5c-abc655c79667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_model = Model(inputs, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fd67f0-b1b2-405f-bb24-9fa8473a96e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18eec7a-a6f2-4a74-b521-9c126785eed0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define our example directories and files\n",
    "base_dir = 'C:\\\\Users\\\\ACER\\\\Capstone\\\\dataset\\\\image_sorted'\n",
    "\n",
    "train_dir = os.path.join( base_dir, 'train')\n",
    "test_dir = os.path.join( base_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fc2b71-e778-4911-9902-2ce1c5f630ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = keras.utils.image_dataset_from_directory(train_dir,\n",
    "                                                         shuffle=True,\n",
    "                                                         batch_size=8,\n",
    "                                                         image_size=(224, 224))\n",
    "\n",
    "test_dataset = keras.utils.image_dataset_from_directory(test_dir,\n",
    "                                                         shuffle=True,\n",
    "                                                         batch_size=8,\n",
    "                                                         image_size=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e3b0ba-8c5a-4ab0-9d73-871b62a5eb4f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Set Optimizer and Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87174834-6b50-486d-993e-d80a75d9867a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "custom_model.compile(optimizer=RMSprop(learning_rate=learning_rate), \n",
    "                     loss=SparseCategoricalCrossentropy(from_logits=True), \n",
    "                     metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad77b64-5285-4031-8f0a-5fcb22359033",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Set Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74cff65-105c-4fbe-acf4-325e3549db46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        '''\n",
    "        Halts the training after reaching 60 percent accuracy\n",
    "        \n",
    "        Args:\n",
    "        epoch (integer) - index of epoch (required but unused in the function definition below)\n",
    "        logs (dict) - metric results from the training epoch\n",
    "        '''\n",
    "        # Check accuracy\n",
    "        if(logs.get('loss') < 0.7):\n",
    "            # Stop if threshold is met\n",
    "            print(\"\\nLoss is lower than 0.7 so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# Instantiate class\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be27075d-e1fa-4b14-bbd7-5a2f3851e086",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b733be6f-e427-4031-a8db-69fd81377864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = custom_model.fit(train_dataset, \n",
    "                           validation_data=test_dataset, \n",
    "                           steps_per_epoch = len(train_dataset),\n",
    "                           epochs = 50,\n",
    "                           validation_steps = len(test_dataset),\n",
    "                           verbose = 2, \n",
    "                           callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f730245-9896-446c-8546-73f503b593aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00daf15e-c620-4f95-9e33-7cdec7566827",
   "metadata": {
    "id": "DkhGMu_bI4M9",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f50735-dcdd-4f8f-b894-3cce98c2c7fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_face(filename, required_size=(224, 224)):\n",
    "    # load image from file\n",
    "    pixels = plt.imread(filename)\n",
    "    \n",
    "    # create the detector, using default weights\n",
    "    detector = MTCNN()\n",
    "    \n",
    "    # detect faces in the image\n",
    "    results = detector.detect_faces(pixels)\n",
    "    \n",
    "    # extract the bounding box from the first face\n",
    "    x1, y1, width, height = results[0]['box']\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    # extract the face\n",
    "    face = pixels[y1:y2, x1:x2]\n",
    "    \n",
    "    # resize pixels to the model size\n",
    "    image = Image.fromarray(face)\n",
    "    image = image.resize(required_size)\n",
    "    face_array = np.asarray(image)\n",
    "    return face_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb645b05-d3bf-46b1-92dc-56a1433057ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Verification Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a611c93b-4448-41f3-afac-49dd8ed08f73",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### With Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531bf833-09e4-48e2-bf12-2cb9bcb4fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"C:\\\\Users\\\\ACER\\\\Capstone\\\\verification\\\\olla1.jpg\" , \n",
    "             \"C:\\\\Users\\\\ACER\\\\Capstone\\\\verification\\\\gita2.jpg\"]\n",
    "\n",
    "faces = [extract_face(f) for f in filenames]\n",
    "samples = np.asarray(faces, \"float32\")\n",
    "samples = preprocess_input(samples, version=2)\n",
    "\n",
    "# perform prediction\n",
    "embeddings = custom_model.predict(samples)\n",
    "thresh = 0.0015\n",
    "score = cosine(embeddings[0], embeddings[1])\n",
    "if score <= thresh:\n",
    "    print( \" >face is a match (%.4f <= %.4f) \" % (score, thresh))\n",
    "else:\n",
    "    print(\" >face is NOT a match (%.4f > %.4f)\" % (score, thresh))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf5fca6-ef88-4595-a3ad-d872ac357519",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Without Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed897cc-85f1-4b23-bcaf-a71be2faf030",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filenames = [\"C:\\\\Users\\\\ACER\\\\Capstone\\\\verification3\\\\irene1.jpg\" , \n",
    "             \"C:\\\\Users\\\\ACER\\\\Capstone\\\\verification3\\\\irene2.jpg\"]\n",
    "\n",
    "faces = [extract_face(f) for f in filenames]\n",
    "samples = np.asarray(faces, \"float32\")\n",
    "samples = preprocess_input(samples, version=2)\n",
    "model = VGGFace(model= \"resnet50\" , include_top=False, input_shape=(224, 224, 3), pooling= \"avg\" )\n",
    "\n",
    "# perform prediction\n",
    "embeddings = model.predict(samples)\n",
    "thresh = 0.5\n",
    "score = cosine(embeddings[0], embeddings[1])\n",
    "if score <= thresh:\n",
    "    print( \" >face is a match (%.3f <= %.3f) \" % (score, thresh))\n",
    "else:\n",
    "    print(\" >face is NOT a match (%.3f > %.3f)\" % (score, thresh))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340dd167-0948-4946-9e5f-5769dc045f59",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae67a384-afbd-478e-b44c-f153d63818f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_model.save('model/model-6-8079.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5736b002-c7e0-47aa-95ea-4ae17f9ce8e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Convert Model to Tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b18e52-37d1-4b3a-ad2d-66b6c66732a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vggface_resnet_converter = tf.lite.TFLiteConverter.from_keras_model(custom_model)\n",
    "vggface_resnet_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "vggface_resnet_tflite = vggface_resnet_converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1011b4-7f5a-4be1-b6aa-89ffd2b87b62",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Save the Tflite File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516569ce-5fa2-47f8-bc5a-e49bbfe96c7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('model/model-6.tflite', 'wb') as f:\n",
    "    f.write(vggface_resnet_tflite)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
