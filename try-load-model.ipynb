{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc60dd4e-1a6e-4fed-a65f-9e1d6bf4345c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d9f88-1a03-4e72-8bc3-715f29c3d08b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7639693f-b7c3-46d4-b58a-51c2450bde14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_vggface\n",
    "from keras_vggface.vggface import VGGFace\n",
    "import keras_vggface.utils\n",
    "from keras_vggface.utils import preprocess_input\n",
    "from keras import Model\n",
    "from keras import layers\n",
    "from keras.layers import Flatten, Input\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdd4e71-b220-4a8e-974f-1b119cec2da7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mtcnn\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166780e-f2be-41c7-896b-1886655c0824",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## VGGFace Model for Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90b9f54-b168-4f3d-b0e0-4b5b74c6bd3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c723e5f-028f-4c17-810f-3f0cfc067845",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3f061a-c14f-4a20-abfa-2cad382b4d09",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Set Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2022e55e-815b-475e-90fe-e85479969e71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    keras.layers.RandomFlip('horizontal'),\n",
    "    keras.layers.RandomRotation(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe223ee-97be-4ac0-9f79-005f5099816a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Choose the Last Layer from Base Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c710b8a7-752b-43e1-8322-2bd0b31c2a29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose `avg_pool` as the last layer of your base model\n",
    "last_layer = model.get_layer('avg_pool')\n",
    "print('last layer output shape: ', last_layer.output_shape)\n",
    "last_output = last_layer.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36628f7-68ed-413f-81c9-c01d16660698",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Build the Arcitecture of Your Model and for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5966703c-ae49-4285-81e9-e007b4702b6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(224, 224, 3))\n",
    "\n",
    "x = data_augmentation(inputs)\n",
    "x = model(x)\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(1024, activation='relu', name='fc1')(x)\n",
    "x = Dense(1024, activation='relu', name='fc2')(x)\n",
    "x = Dense(1024, activation='relu', name='fc3')(x) \n",
    "x = Dense(1024, activation='relu', name='fc4')(x) \n",
    "x = Dense(512, activation='relu', name='fc5')(x)\n",
    "x = Dense(512, activation='relu', name='fc6')(x) \n",
    "x = Dense(512, activation='relu', name='fc7')(x) \n",
    "x = Dense(256, activation='relu', name='fc8')(x)\n",
    "x = Dense(256, activation='relu', name='fc9')(x)\n",
    "out = Dense(105, name='classifier')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e048641-a5fe-4d70-9719-4a1822596926",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_model = Model(inputs, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59724ce8-25f0-4d25-b0cb-7fc5350e1747",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Load Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b684b0-8231-4b1f-a8b0-b036663e5abb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "custom_model.load_weights('./model/model-6-8079.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf630ae-3daf-4a26-b868-90b23954c094",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Make a Function for Face Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7df49ed-3042-4e6d-8825-688568d29442",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_face(filename, required_size=(224, 224)):\n",
    "    # load image from file\n",
    "    pixels = plt.imread(filename)\n",
    "    \n",
    "    # create the detector, using default weights\n",
    "    detector = MTCNN()\n",
    "    \n",
    "    # detect faces in the image\n",
    "    results = detector.detect_faces(pixels)\n",
    "    \n",
    "    # extract the bounding box from the first face\n",
    "    x1, y1, width, height = results[0]['box']\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    # extract the face\n",
    "    face = pixels[y1:y2, x1:x2]\n",
    "    \n",
    "    # resize pixels to the model size\n",
    "    image = Image.fromarray(face)\n",
    "    image = image.resize(required_size)\n",
    "    face_array = np.asarray(image)\n",
    "    return face_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d684a66-3c09-48fa-a3f4-cdd7d29c10dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Verification Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c315fa5f-dee9-423f-be8a-bf457011ca82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filenames = [\"C:\\\\Users\\\\ACER\\\\Capstone\\\\verification3\\\\bruno1.jpg\", \n",
    "             \"C:\\\\Users\\\\ACER\\\\Capstone\\\\verification3\\\\bruno2.jpg\"] #Try verification\n",
    "\n",
    "faces = [extract_face(f) for f in filenames]\n",
    "samples = np.asarray(faces, \"float32\")\n",
    "samples = preprocess_input(samples, version=2)\n",
    "\n",
    "# perform prediction\n",
    "embeddings = custom_model.predict(samples)\n",
    "thresh = 0.002 #Treshold 0.002 from \n",
    "score = cosine(embeddings[0], embeddings[1])\n",
    "if score <= thresh:\n",
    "    print( \" >face is a match (%.3f <= %.3f) \" % (score, thresh))\n",
    "else:\n",
    "    print(\" >face is NOT a match (%.3f > %.3f)\" % (score, thresh))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e570d06-0a8f-4f55-a214-bc1693ac59f9",
   "metadata": {},
   "source": [
    "### Verification with Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020bffa5-ee49-424f-979a-5dde3c1d2a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filenames = [\"C:\\\\Users\\\\ACER\\\\Capstone\\\\verification3\\\\bruno1.jpg\", \n",
    "             \"C:\\\\Users\\\\ACER\\\\Capstone\\\\verification3\\\\bruno2.jpg\"]\n",
    "\n",
    "faces = [extract_face(f) for f in filenames]\n",
    "samples = np.asarray(faces, \"float32\")\n",
    "samples = preprocess_input(samples, version=2)\n",
    "model = VGGFace(model= \"resnet50\" , include_top=False, input_shape=(224, 224, 3), pooling= \"avg\" )\n",
    "\n",
    "# perform prediction\n",
    "embeddings = model.predict(samples)\n",
    "thresh = 0.5\n",
    "score = cosine(embeddings[0], embeddings[1])\n",
    "if score <= thresh:\n",
    "    print( \" >face is a match (%.3f <= %.3f) \" % (score, thresh))\n",
    "else:\n",
    "    print(\" >face is NOT a match (%.3f > %.3f)\" % (score, thresh))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
